{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cef39b34-128b-43d7-9317-6a7e07603d3f",
   "metadata": {},
   "source": [
    "# 10 dimensions of Social Exchange\n",
    "\n",
    "Goal of this notebook is to get the 10 dimensions of social exchange embeddings for the inaturalist text. \n",
    "Actually, this runs super fast on two gpus (maybe 1-2 minutes for all dimensions for roughly 10000 instances each). \n",
    "TODO: Get data in right format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02d2f563-811a-4de9-b292-dcebcacb5f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bkomander/10dimensions/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import os\n",
    "from os.path import join\n",
    "import sys\n",
    "from transformers import BertTokenizer,BertForSequenceClassification\n",
    "from transformers.optimization import AdamW\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ce63f4b-2f2a-442d-9f3e-13e36b2fd317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting some variables:\n",
    "# set cuda (not advisable to run on cpu)\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "BERT_MODEL = 'bert-base-cased' # BERT model type\n",
    "CACHE_DIR = 'cache/' # where BERT will look for pre-trained models to load parameters from\n",
    "\n",
    "num_labels = 2\n",
    "\n",
    "OUTPUT_MODE = 'classification'\n",
    "CONFIG_NAME = \"config.json\"\n",
    "WEIGHTS_NAME = \"pytorch_model.bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0943ff22-5d03-413e-b156-812a9e3826fa",
   "metadata": {},
   "source": [
    "Can play around with batch size. For current config (although quite small) batch size of 1000 only uses 30 percent of gpu memory\n",
    "\n",
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b9bf74e-74ca-4629-99dc-df632c35b465",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/bkomander/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "idx_list = []\n",
    "sentence_list = []\n",
    "text_array = np.load(\"data/naturalist_title_text.npy\", allow_pickle=True)\n",
    "for idx, text in enumerate(text_array):\n",
    "    sentences = sent_tokenize(text)\n",
    "    idx_list.extend([idx] * len(sentences))\n",
    "    sentence_list.extend(sentences)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "231affc7-1410-4399-9b18-8b0bfe4741ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data this will probably change \n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL, do_lower_case=False)\n",
    "test_array = np.load(\"data/naturalist_question.npy\", allow_pickle=True)\n",
    "encodings = tokenizer(sentence_list, padding=True, add_special_tokens=True, truncation=True, return_tensors='pt').to(device)\n",
    "data_set = TensorDataset(encodings['input_ids'], encodings['attention_mask'])\n",
    "test_loader = DataLoader(data_set,  batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d390a9d-e378-45f5-96c5-873ada75b4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "835it [17:49,  1.28s/it]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "835it [26:04,  1.87s/it]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "835it [27:09,  1.95s/it]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "835it [27:19,  1.96s/it]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "835it [27:01,  1.94s/it]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "835it [27:27,  1.97s/it]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "835it [27:22,  1.97s/it]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "835it [27:35,  1.98s/it]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "835it [27:30,  1.98s/it]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "835it [27:19,  1.96s/it]\n"
     ]
    }
   ],
   "source": [
    "#dimensions = [\"conflict\",\"fun\", \"identity\"]\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*Some weights of BertForSequenceClassification were not initialized.*\")\n",
    "dimensions = [\"conflict\",\"fun\", \"identity\", \"knowledge\", \"power\", \"respect\", \"romance\", \"similarity\", \"social_support\", \"trust\"]\n",
    "results = {}\n",
    "for dim in dimensions:\n",
    "    OUTPUT_DIR = 'weights/BERT/%s' %dim \n",
    "    output_model_file = os.path.join(OUTPUT_DIR, WEIGHTS_NAME)\n",
    "    model = BertForSequenceClassification.from_pretrained(BERT_MODEL,cache_dir=CACHE_DIR, num_labels=num_labels)\n",
    "    model.load_state_dict(torch.load(output_model_file))\n",
    "    if torch.cuda.is_available():\n",
    "        #print('CUDA devices:', torch.cuda.device_count())  # This should print 2 if both GPUs are available\n",
    "        device = torch.device(\"cuda\")\n",
    "    \n",
    "        # Data Parallelism\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            #print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "            model = torch.nn.DataParallel(model)\n",
    "        model.to(device) \n",
    "    # run model\n",
    "    preds = []\n",
    "    for step, batch in tqdm(enumerate(test_loader)):\n",
    "        inputs, att_mask = batch\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs, attention_mask=att_mask)\n",
    "            probabilities = torch.softmax(outputs.logits, dim=1)\n",
    "            positive_class_probs = probabilities[:, 1].detach().cpu().numpy()\n",
    "            preds.append(positive_class_probs)\n",
    "    results[dim] = np.concatenate(preds)\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce88d700-5c54-44e6-bd53-22a8a444f6f9",
   "metadata": {},
   "source": [
    "runs a bit long but thats okay. New thing I have to consider: Mean Across all the sentences:\n",
    "Actually not using the GPU to its fullest, but at this stage this is okay.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfa052aa-2716-4fff-8ee2-c41986dd6b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "df.loc[:, \"index\"] = idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ce89e67-11c9-4228-be51-03d8f8e79abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/inat_10dims.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d9dc8d6-6e59-4511-a9f7-4440e3c4c7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df = df.groupby('index')[df.columns].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "125bf9da-0a25-4341-a3db-3e43a0311c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df.drop(columns=\"index\").to_csv(\"data/reduced_inat_10dims.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
